---
title: "Challenge B"
author: "Sharanya Pillai & Jonas Gathen"
date: "26 November 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

install_load <- function (package1, ...)  {   

   # convert arguments to vector
   packages <- c(package1, ...)

   # start loop to determine if each package is installed
   for(package in packages){

       # if package is installed locally, load
       if(package %in% rownames(installed.packages()))
          do.call('library', list(package))

       # if package is not installed locally, download, then load
       else {
          install.packages(package)
          do.call("library", list(package))
       }
   } 
}
install_load("caret", 
             "randomForest",
             "tidyverse",
             "np",
             "reshape2",
             "VIM",
             "stargazer",
             "xtable",
             "data.table",
             "gridExtra")
```

The Github repository for this analysis can be found [here](https://github.com/jgathen/R_prog_ChallengeB/).

The associated SSH-key is: git@github.com:jgathen/R_prog_ChallengeB.git

The associated HTTPS-key is: https://github.com/jgathen/R_prog_ChallengeB.git

## Task 1B: Fitting a random forest to predict housing prices

### Step 1

We are choosing a random forest. The following description and intuition of the method is mostly based on Hastie, Tibshirani & Friedman (2008) and James, Witten, Hastie & Tibshirani (2014).
Random forests are based on tree-based methods. Tree-based methods work through iteratively segmenting the space of possible predictions based on certain splitting rules. The collection of these splitting rules can then be summarized in a tree-form. Random forests represent a way in which to combine the predictions of multiple trees to form one combined prediction.

It is especially linked to the method  of bagging. The essential idea in bagging is to reduce variance by averaging many noisy but approximately unbiased models. Since trees are notoriously noisy, they benefit greatly from the averaging. Each tree generated in bagging is identically distributed, but not necessarily independent. This has the unfortunate consequence that even for many trees, the variance of the average of trees will face a lower bound that is given by the positive pairwise correlation of trees. 

The idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved by the following general algorithm:

+ Choose the size of your forest (i.e. how many trees you want to predict). 
+ Then for each tree, draw a bootstrap sample from the training data. 
+ With this sample, select a number m of variables at random from the total number of variables p.
+ Pick the best variable among the number of variables that were randomly drawn (based on splitting criterium) and split accordingly.
+ Again, randomly draw a prespecified number of variables from the total, pick the best variable and split. Repeat until a specified minimum node size is reached.
+ Save the resulting tree and repeat procedure to get the remaining trees.
+ The random forest predictor is given by the average of the trees. 

The correlation between trees can be controlled by m. The smaller m, the smaller the correlation will be. However, with large p (the total number of variables) and many non-relevant variables, a small m can lead to weak performance. This is not likely to be a problem in this case here, because p is not very large and we actually have a large number of important variables. As a practical advice, for regression, the default value for m is p/3 and the minimum node size is five; this should be fine-tuned however and can vary significantly from one problem to another.

### Step 2: Train the chosen technique on the training data

We read the revised training data from the last challenge and train the model using the above-outlined algorithm for a random forest. We stick with the recommended default of 500 trees per forest and a terminal node size of 5. We also use the default value for m as p/3. Usually, we would call the random forest algorithm from within the caret-package to fine-tune the parameter m by using 5- or 10-fold cross-validation. This takes too long, though; and the exercise is only meant to illustrate anyway. 

```{r train_randomforest, cache=TRUE}
training <- read.csv(file = "training_final_ChallengeA.csv", header = T)
random_forest_model1 <- randomForest(data=training, SalePrice~.-Id, ntree = 500, nodesize = 5)
```

### Step 3: Make predictions on the test data, and compare them to the predictions of a linear regression of your choice.

We read the revised test dataset from the previous challenge. We use our trained model from above to make predictions on the test data. We then compare these predictions to the predictions obtained in the previous challenge, where we fitted a final model with Least Angle Regression. 

```{r predictions_1B, cache=TRUE}
test <- read.csv(file = "test_final_ChallengeA.csv", header = TRUE)
rf_predictions <- predict(random_forest_model1, test)
lar_predictions <- read.csv(file = "final_predictions_ChallengeA.csv", header = TRUE)
compare_predictions <- cbind(lar_predictions,rf_predictions)
```

We can now compare the predictions (without being able to look at the true values). This is best illustrated by plotting the predictions against each other. The figure below shows that they are fairly close (r = 0.88). Main differences arise for extreme observations for which both models predict lower or higher prices on average. In general, the Least Angle Regression predicts more extreme values; for prices below average, the Least Angle regression model predicts much lower prices, for prices above average, the Least Angle regression predicts higher prices.

```{r compare_predictions, results='asis', echo=FALSE, cache=TRUE}
ggplot(data = compare_predictions) + 
  geom_point(aes(x = log(SalePrice), y = log(rf_predictions)), alpha = 0.4) + # Plot logs of predicted Sale prices
  stat_function(fun = function(x) x) + # Plot against 45 degree line
  geom_vline(xintercept=mean(log(compare_predictions$SalePrice)), color = "red") + # Plot mean of LAR predictions
  geom_hline(yintercept=mean(log(compare_predictions$rf_predictions)), color = "red") + # Plot mean of RF predictions
  labs(title="Comparing model predictions", 
       x ="Least Angle Regression Predictions (in logs)", 
       y = "Random Forest Predictions (in logs)") +
  annotate("text", x = 13, y = 14,label = paste("r = ", round(cor(compare_predictions$SalePrice, compare_predictions$rf_predictions), digits = 2)), size = 6) +
  annotate("text", x = 9.5, y = 9.7, angle = 32, label = "45Â° line") +
  annotate("text", x = 13.7, y = 12.2, label = "Mean of predictions")
```


## Task 2B - Overfitting in Machine Learning (continued) - 1 point for each step

We can briefly create the data we need for this exercise.

```{r simulate_data, cache=TRUE}
set.seed(1234)
nsims <- 150 # Number of simulations
e <- rnorm(n = nsims, mean = 0, sd = 1) # Draw 150 errors from a normal distribution
x <- rnorm(n = nsims, mean = 0, sd = 1) # Draw 150 x obs. from a normal distribution 
y <- x^3+e # generate y following (T)
df <- data.frame(y,x)

df$ID <- c(1:150)
training2 <- df[df$ID %in% sample(df$ID, size = 120, replace = F), ] # Get training set of size 120
test2 <- df[!(df$ID %in% training2$ID), ] # Get remaining test dataset
df$training <- (df$ID %in% training2$ID) # Create variable specifying whether obs. is in test or training set
```


### Step 1: Estimate a low-flexibility local linear model on the training data

```{r low_flex_ll, cache=TRUE}
ll.fit.lowflex <- npreg(training2, formula = y ~ x, method = "ll", bws = 0.5)
```

### Step 2: Estimate a high-flexibility local linear model on the training data. 

```{r high_flex_ll, cache=TRUE}
ll.fit.highflex <- npreg(training2, formula = y ~ x, method = "ll", bws = 0.01)
```

### Step 3: Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex, on only the training data.

```{r, cache=TRUE, tidy=TRUE}
# Get estimates of both models for training2 data
lowflex_estimates <- data.frame(y_estimates_lowflex = ll.fit.lowflex$mean, 
                                y = training2$y, 
                                x = ll.fit.lowflex$eval)
highflex_estimates <- data.frame(y_estimates_highflex = ll.fit.highflex$mean, 
                                 y = training2$y, 
                                 x = ll.fit.highflex$eval)
combined_estimates <- merge(lowflex_estimates, highflex_estimates)

ggplot(data = combined_estimates) + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = y_estimates_lowflex, color = "red")) +
  geom_line(aes(x = x, y = y_estimates_highflex, color = "darkblue")) +
  stat_function(fun = function(x) x^3) +
  scale_color_discrete(name = "Local Linear Regression", 
                       labels = c("Highflex", "Lowflex"))
```

### Step 4 - Between the two models, which predictions are more variable? Which predictions have the least bias?

In machine learning, we look at models from the perspective of the bias-variance trade-off. We can decompose the test error into the irreducible error - i.e. the error of the true model -, the squared bias and the variance. Bias gives the difference between the true mean of the model and the estimated mean and variance gives the variability of a model prediction for a given data point.

For the training data, we don't have the bias-variance trade-off and we could theoretically train a model with zero training error (and thus zero bias and variance). Here, we see a positive training error; the more flexible model has a lower bias and predictions are more variable (though the variance of the training error is still lower). 

### Step 5 - Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex now using the test data. Which predictions are more variable? What happened to the bias of the least biased model?

```{r, cache=TRUE, tidy=TRUE}
# Get predictions of both models for test2 data
lowflex_predictions <- predict(ll.fit.lowflex, newdata = test2)
highflex_predictions <- predict(ll.fit.highflex, newdata = test2)
combined_predictions <- cbind(test2, lowflex_predictions, highflex_predictions)

ggplot(data = combined_predictions) + geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = lowflex_predictions, color = "red")) + # Plot predictions from lowflex
  geom_line(aes(x = x, y = highflex_predictions, color = "darkblue")) + # Plot predictions from highflex
  stat_function(fun = function(x) x^3) + 
  scale_color_discrete(name = "Local Linear Regression", labels = c("Highflex", "Lowflex"))
```

Again, predictions for the highly flexible model are more variable. The bias of both models is fairly similar, with the highflex model maybe slightly better. As expected, the highflex model had a much lower bias on the training data, which points to overfitting. 

### Step 6 - Create a vector of bandwidth going from 0.01 to 0.5 with a step of 0.001

```{r, cache=TRUE}
bandwidth_vector <- seq(0.01,0.5,0.001)
```

### Step 7 - Estimate a local linear model y ~ x on the training data with each bandwidth.

We can either do this via looping or in vectorized form. We don't expect there to be any differences in computation time however, because we are running a different regression for each bandwidth. If we would apply the exact same function to each of the vector's elements, then vectorization could save a lot of time.

```{r, cache=TRUE}
run_ll <- function(bandwidth){
  npreg(training2, formula = y ~ x, method = "ll", bws = bandwidth)
}
ll_models <- lapply(X = bandwidth_vector, FUN = run_ll)
```

### Step 8 - Compute for each bandwidth the MSE on the training data.

In the next step, we can just extract the already computed MSE from our model output. Again, vectorizing or looping take the same amount of time.

```{r, cache=TRUE}
MSE_training <- sapply(c(1:length(bandwidth_vector)), FUN = function(i) ll_models[[i]]$MSE)
```

### Step 9 - Compute for each bandwidth the MSE on the test data.

Again, a simple loop will do.

```{r, cache=TRUE}
MSE_test <- c() 
for(i in 1:length(bandwidth_vector)){
  ll_model_predictions <- predict(ll_models[[i]], newdata = test2) # Get prediction for given bandwidth
  MSE_test[i] <- mean((test2$y-ll_model_predictions)^2) # Compute mean squared error for given bandwidth
}
```

### Step 10 - Draw on the same plot how the MSE on training data, and test data, change when the bandwidth increases. Conclude.

```{r, cache=TRUE}
MSE <- data.frame(bandwidth = bandwidth_vector, MSE_training, MSE_test) # Combine in one dataset
MSE_long <- melt(data = MSE, id.vars = c("bandwidth"), value.name = "MSE") # Get in long format
ggplot() + 
  geom_line(data = MSE_long, aes(x = bandwidth, y = MSE, group = variable, color = variable)) + 
  scale_color_discrete(name = "Mean Squared Error", labels = c("Training","Test"))
```

The graph (almost) speaks for itself. Choosing a low bandwidth - i.e. fitting a highly flexible model - leads to a MSE of almost zero for the training data, but to a high MSE on the test data. Decreasing the flexibility of the model leads to an increase in MSE for the training data, but to a decrease in MSE for the test data. While the MSE is monotonically increasing with lower flexibility for the training data, there is an optimal level of flexibility for the test data, as we would expect. At some point, lowering the flexibility will underestimate the complexity of the true model. Thus, from this graph, we would like to train a model with a bandwidth of around 0.32.


## Task 3B: Privacy regulation compliance in France

### Step 1 - Import the CIL dataset from the Open Data Portal. (1 point)

In order to guarantee reproducibility of the research, we first download the data into the Github repository and then load the dataset from there. We briefly check whether all variables are read correctly and whether we have issues with missing values. We delete all 302 observations for which we don't have the SIREN number.

```{r, echo=TRUE, results='hide'}
cil <- read.csv("cil.csv", header=TRUE, sep = ";", na.strings = c(""," ",NA,".",",",";")) 
str(cil)

# Check missings
aggr(cil, plot = FALSE)
cil <- cil %>% filter(!is.na(Siren)) # Delete all observations for which we don't observe SIREN number
```

### Step 2 - Show a (nice) table with the number of organizations that have nominated a CIL per department. HINT : A department in France is uniquely identified by the first two digits of the postcode. (1 point)

We first create a variable for the department by taking the first two characters from the postal code variable. Then, we create a new dataset that only includes unique Sirens per department and count the number of CIL delegates by the same company that sends multiple CILs per department. At last, we count how many different organizations have nominated at least one CIL per department. 

```{r, tidy=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
cil$Department <- as.numeric(substr(cil$Code_Postal, start = 1,stop = 2)) # Get department variable
cil_by_Siren_by_dpt <- cil %>% count(Department,Siren) # Number of nominations per Siren number per department
names(cil_by_Siren_by_dpt) <- c("Department", "Siren", "Nominations") # Rename variables
cil_by_dpt <- cil_by_Siren_by_dpt %>% count(Department) # Number of Siren numbers per department
names(cil_by_dpt) <- c("Department", "Nominations") # Rename variables
```

```{r, echo = FALSE, results='asis'}
summaryfunction <- function(x){
  if( is.numeric(x)!=TRUE) {stop("Supplied X is not numeric")}
  mysummary = data.frame(
            "Obs." = length(x),
            "Min." = as.numeric( min(x)),
            "First Q." = quantile(x)[[2]],
            "Median" = median(x),
            "Mean" = mean(x),
            "Third Q." = quantile(x)[[4]],
            "Max." = max(x))
  return(mysummary)
}

print(xtable(summaryfunction(cil_by_Siren_by_dpt$Nominations), caption = "Number of nominations per Siren per Department"),
      comment=FALSE, include.rownames = FALSE)
print(xtable(summaryfunction(cil_by_dpt$Nominations), caption = "Number of organizations per department"), 
      comment=FALSE, include.rownames = FALSE)
```

### Step 3 - Merge the information from the SIREN dataset into the CNIL data. Explain the method you use. HINT : In the SIREN dataset, there are some rows that refer to the same SIREN number, use the most up to date information about each company. (2 points)

Reproducibility with such large datasets is problematic, because it is not possible to store such a large dataset on Github and the link for the dataset might change subsequently. Thus, we download and locally save the dataset to make sure that we always keep the version for which the analysis was done. In the following, we assume that the dataset is locally stored in the folder of the RProject. 

We proceed in three steps. First, we load the SIREN dataset. We are only interested in the organizations/firms that have a CIL representative. Unfortunately, we still have to read the entire dataset, because there are duplicate SIREN numbers in the SIREN dataset. To save memory, we could load this in chunks and start deleting all Siren numbers which are not in the CNIL dataset. This is a memory vs. computation time trade-off. Here, we choose in favor of computation time and load everything at once. We can save computation time and memory by preselecting variables of interest.

After having loaded the dataset, we convert the column classes into the classes we need. Then, we group by SIREN number and delete all duplicate SIREN numbers except the most recent entries given by the DATEMAJ column. 

In the third step, we merge (or left_join) the amended SIREN dataset with the CNIL dataset. This adds company-related variables for all observations in the CNIL dataset for which there is data in the SIREN dataset.


```{r, cache=TRUE, tidy=TRUE, warning=FALSE}
#column_names_siren <- names(fread(file = "siren.csv", header = TRUE, fill = TRUE, sep = ";", nrows = 0))
#variable_selection <- c("SIREN","DEPET","EFETCENT","EFENCENT","DATEMAJ") # Make selection of 5 variables

#system.time(siren <- fread(file = "siren.csv", header = TRUE, fill = TRUE, sep = ";", select = variable_selection,
#               na.strings = c("",NA,"NN")))

#siren[,c(1:4)] <- lapply(siren[,c(1:4)], function(x) as.numeric(as.character(x))) # Convert var. to integer
#siren$DATEMAJ <- gsub("T", " ", siren$DATEMAJ) # Change format of date/time so that R understands its a date/time
#siren$DATEMAJ <- as.POSIXct(siren$DATEMAJ) # Convert character to date format

#siren <- siren %>% group_by(SIREN, DEPET) %>% filter(DATEMAJ == max(DATEMAJ)) %>% ungroup() # Delete duplicates

#cil_final <- left_join(cil_by_Siren_by_dpt, siren, by = c("Siren" = "SIREN", "Department" = "DEPET"))
```


### Step 4 - Plot the histogram of the size of the companies that nominated a CIL. Comment. (1 points)

The size of the company can be measured in numerous ways. We proxy the size of the companies that nominated a CIL by their number of employees. For this we have data both on the specific department-level establishment and the entire company. We compare both of them. 

```{r, tidy=TRUE, message=FALSE, warning=FALSE}
#plot_size_dpt <- ggplot(data = cil_final) + geom_histogram(aes(log(EFETCENT)))
#plot_size_total <- ggplot(data = cil_final) + geom_histogram(aes(log(EFENCENT)))
#grid.arrange(plot_size_dpt, plot_size_total, ncol = 2,
#             top = paste0("Size of companies that"," nominated a CIL representative"),
#             bottom = textGrob(paste0("Measured by number of workers"," (in logs).",
#                                      " Size of establishment (left) and",
#                                      " size of the entire company (right)."), 
#             gp=gpar(fontsize=9,font=8)))
```

The distribution is unimodal with the mode at a value of about 15 workers (note that the variable is discretized and all figures are rounded down). There is a long right tail (note that the figure is log-transformed), so some establishments and companies have many more workers. For example, log(8) is equal to almost 3000 workers. Naturally, companies have more workers than establishments. It would be helpful to compare these figures to the entire distribution of French establishments and companies. 

